{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and validation dataloaders are ready!\n",
      "[Train Dataloader 1] 샘플 배치 로드 성공!\n",
      "이미지 텐서 크기: torch.Size([64, 3, 256, 256])\n",
      "마스크 텐서 크기: torch.Size([64, 256, 256])\n",
      "[Train Dataloader 1] 마스크의 고유 클래스 ID: tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "[Train Dataloader 2] 샘플 배치 로드 성공!\n",
      "이미지 텐서 크기: torch.Size([64, 3, 256, 256])\n",
      "마스크 텐서 크기: torch.Size([64, 256, 256])\n",
      "[Train Dataloader 2] 마스크의 고유 클래스 ID: tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "[Train Dataloader 3] 샘플 배치 로드 성공!\n",
      "이미지 텐서 크기: torch.Size([64, 3, 256, 256])\n",
      "마스크 텐서 크기: torch.Size([64, 256, 256])\n",
      "[Train Dataloader 3] 마스크의 고유 클래스 ID: tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "[Train Dataloader 4] 샘플 배치 로드 성공!\n",
      "이미지 텐서 크기: torch.Size([64, 3, 256, 256])\n",
      "마스크 텐서 크기: torch.Size([64, 256, 256])\n",
      "[Train Dataloader 4] 마스크의 고유 클래스 ID: tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "[Validation Dataloader] 샘플 배치 로드 성공!\n",
      "이미지 텐서 크기: torch.Size([64, 3, 256, 256])\n",
      "마스크 텐서 크기: torch.Size([64, 256, 256])\n",
      "[Validation Dataloader] 마스크의 고유 클래스 ID: tensor([0, 1, 2, 3, 4, 5, 6, 7])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import glob\n",
    "\n",
    "class CustomSegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_paths = glob.glob(os.path.join(image_dir, '*.jpg'))\n",
    "        self.mask_paths = [os.path.join(mask_dir, f\"Mask_{os.path.basename(p).replace('.jpg', '.png')}\") for p in self.image_paths]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 이미지 및 마스크 경로 가져오기\n",
    "        image_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path)\n",
    "\n",
    "        # 이미지와 마스크에 동일한 변환 적용\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = mask.resize((256, 256), resample=Image.NEAREST)\n",
    "        \n",
    "        # 마스크는 PIL 이미지를 NumPy로 변환 후 텐서로 변환\n",
    "        mask_np = np.array(mask, dtype=np.int64)\n",
    "        mask_tensor = torch.from_numpy(mask_np).long()\n",
    "\n",
    "        return image, mask_tensor\n",
    "\n",
    "# 변환 설정 (리사이즈 포함)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # 이미지 크기 조정\n",
    "    transforms.ToTensor(),  # 텐서로 변환\n",
    "])\n",
    "\n",
    "# 원본 이미지와 마스크 이미지 경로 설정\n",
    "train_image_dirs = ['./Surface_1', './Surface_2', './Surface_3', './Surface_4']  # 학습용 디렉토리\n",
    "val_image_dir = './Surface_5'  # 검증용 디렉토리\n",
    "train_mask_dirs = ['./Surface_1_mask', './Surface_2_mask', './Surface_3_mask', './Surface_4_mask']\n",
    "val_mask_dir = './Surface_5_mask'\n",
    "\n",
    "# 각 학습 디렉토리에 대해 데이터로더 생성\n",
    "train_dataloaders = [DataLoader(CustomSegmentationDataset(image_dir, mask_dir, transform=transform), batch_size=64, shuffle=True) \n",
    "                     for image_dir, mask_dir in zip(train_image_dirs, train_mask_dirs)]\n",
    "\n",
    "# 검증 데이터셋 및 데이터로더 준비\n",
    "val_dataset = CustomSegmentationDataset(val_image_dir, val_mask_dir, transform=transform)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"Train and validation dataloaders are ready!\")\n",
    "\n",
    "# 데이터 로더에서 샘플 확인\n",
    "for i, dataloader in enumerate(train_dataloaders):\n",
    "    try:\n",
    "        images, masks = next(iter(dataloader))\n",
    "        print(f\"[Train Dataloader {i + 1}] 샘플 배치 로드 성공!\")\n",
    "        print(\"이미지 텐서 크기:\", images.size())\n",
    "        print(\"마스크 텐서 크기:\", masks.size())\n",
    "        print(f\"[Train Dataloader {i + 1}] 마스크의 고유 클래스 ID:\", torch.unique(masks))\n",
    "    except Exception as e:\n",
    "        print(f\"Train Dataloader {i + 1} 오류 발생:\", e)\n",
    "\n",
    "try:\n",
    "    images, masks = next(iter(val_dataloader))\n",
    "    print(\"[Validation Dataloader] 샘플 배치 로드 성공!\")\n",
    "    print(\"이미지 텐서 크기:\", images.size())\n",
    "    print(\"마스크 텐서 크기:\", masks.size())\n",
    "    print(\"[Validation Dataloader] 마스크의 고유 클래스 ID:\", torch.unique(masks))\n",
    "except Exception as e:\n",
    "    print(\"Validation 데이터 로더 오류 발생:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset 이미지 개수: 37607\n",
      "Validation dataset 이미지 개수: 8792\n"
     ]
    }
   ],
   "source": [
    "# 학습 데이터셋의 이미지 개수 확인\n",
    "total_train_samples = sum(len(dataloader.dataset) for dataloader in train_dataloaders)\n",
    "print(\"Train dataset 이미지 개수:\", total_train_samples)\n",
    "\n",
    "# 검증 데이터셋의 이미지 개수 확인\n",
    "print(\"Validation dataset 이미지 개수:\", len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "# 경량화된 ASPP 모듈\n",
    "class LightweightASPP(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(LightweightASPP, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=3, dilation=3)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=6, dilation=6)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.conv_pool = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.bn_pool = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.conv_out = nn.Conv2d(out_channels * 4, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = F.relu(self.bn1(self.conv1(x)))\n",
    "        x2 = F.relu(self.bn2(self.conv2(x)))\n",
    "        x3 = F.relu(self.bn3(self.conv3(x)))\n",
    "\n",
    "        x4 = self.pool(x)\n",
    "        x4 = F.relu(self.bn_pool(self.conv_pool(x4)))\n",
    "        x4 = F.interpolate(x4, size=x3.size()[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        x = torch.cat((x1, x2, x3, x4), dim=1)\n",
    "        x = self.conv_out(x)\n",
    "        return x\n",
    "\n",
    "# 경량화된 DeepLabV3 모델\n",
    "class LightweightDeepLabV3(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(LightweightDeepLabV3, self).__init__()\n",
    "\n",
    "        # MobileNetV2 백본 사용\n",
    "        backbone = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "        self.backbone = nn.Sequential(\n",
    "            backbone.features,\n",
    "            nn.Conv2d(1280, 320, kernel_size=1)  # 마지막 레이어를 압축\n",
    "        )\n",
    "\n",
    "        # 경량화된 ASPP 모듈\n",
    "        self.aspp = LightweightASPP(in_channels=320, out_channels=128)\n",
    "\n",
    "        # 피처 조정을 위한 추가 Conv 레이어\n",
    "        self.conv1 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # 최종 출력 클래스 수에 맞추기\n",
    "        self.conv2 = nn.Conv2d(128, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_size = x.size()[2:]\n",
    "\n",
    "        x = self.backbone(x)\n",
    "\n",
    "        x = self.aspp(x)\n",
    "\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        x = F.interpolate(x, size=x_size, mode='bilinear', align_corners=False)\n",
    "        return x\n",
    "\n",
    "# GPU 또는 CPU에 모델 로드\n",
    "model = LightweightDeepLabV3(num_classes=8)  # 클래스 수에 맞게 설정\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Class names definition\n",
    "CLASS_NAMES = [\"Background\", \"Sidewalk\", \"Braille Guide Blocks\", \"Roadway\", \"Alley\", \"Bike Lane\", \"Caution Zone\", \"Cross walk\"]\n",
    "\n",
    "# 성능 지표를 CSV 파일에 저장하는 함수 (클래스별 mIoU 추가, 배경 클래스 제외)\n",
    "def save_metrics_to_csv(epoch, train_loss, val_loss, train_miou, val_miou, class_train_iou, class_val_iou, filename=\"resize_metrics.csv\"):\n",
    "    try:\n",
    "        file_exists = os.path.isfile(filename)\n",
    "        \n",
    "        with open(filename, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            if not file_exists:\n",
    "                # CSV 헤더에 클래스별 mIoU 추가 (배경 제외)\n",
    "                header = [\"Epoch\", \"Train Loss\", \"Val Loss\", \"Train mIoU\", \"Val mIoU\"] + \\\n",
    "                         [f\"Train IoU - {name}\" for name in CLASS_NAMES[1:]] + \\\n",
    "                         [f\"Val IoU - {name}\" for name in CLASS_NAMES[1:]]\n",
    "                writer.writerow(header)\n",
    "            \n",
    "            # 배경 클래스를 제외한 클래스별 mIoU와 함께 데이터를 작성\n",
    "            row = [epoch, train_loss, val_loss, train_miou, val_miou] + \\\n",
    "                  class_train_iou + class_val_iou\n",
    "            writer.writerow(row)\n",
    "        print(f\"Metrics saved successfully to {filename}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save metrics to CSV: {e}\")\n",
    "\n",
    "# Function to save model checkpoints\n",
    "def save_checkpoint(epoch, model, optimizer, checkpoint_dir, filename=\"last_checkpoint.pth\"):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "    checkpoint_filename = os.path.join(checkpoint_dir, filename)\n",
    "    torch.save(checkpoint, checkpoint_filename)\n",
    "    print(f\"Checkpoint saved at epoch {epoch} as {checkpoint_filename}.\")\n",
    "\n",
    "# Function to load model checkpoints\n",
    "def load_checkpoint(filename, model, optimizer=None):\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    print(f\"Checkpoint loaded from epoch {epoch}.\")\n",
    "    return epoch\n",
    "\n",
    "# Function to calculate IoU per class\n",
    "def calculate_iou_per_class(pred, target, num_classes):\n",
    "    iou_per_class = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = (pred_inds & target_inds).sum().float().item()\n",
    "        union = (pred_inds | target_inds).sum().float().item()\n",
    "        if union == 0:\n",
    "            iou_per_class.append(float('nan'))\n",
    "        else:\n",
    "            iou_per_class.append(intersection / union)\n",
    "    return iou_per_class\n",
    "\n",
    "# 모델 훈련 함수 (CSV에 클래스별 mIoU 저장 추가)\n",
    "def train_model_with_csv_and_checkpoint(model, train_dataloader, val_dataloader, criterion, optimizer, num_classes, num_epochs=25, device='cuda:0', csv_filename=\"resize_metrics.csv\", checkpoint_dir=\"./resize_checkpoints\", checkpoint_file=None):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    model.to(device)\n",
    "    print(\"모델 장치:\", next(model.parameters()).device)\n",
    "\n",
    "    best_val_miou = -1\n",
    "    start_epoch = 0\n",
    "    if checkpoint_file and os.path.isfile(checkpoint_file):\n",
    "        start_epoch = load_checkpoint(checkpoint_file, model, optimizer)\n",
    "        print(f\"체크포인트 로드, {start_epoch + 1} 에폭부터 재개합니다.\")\n",
    "    else:\n",
    "        print(\"체크포인트가 없거나 파일을 찾을 수 없습니다. 처음부터 학습을 시작합니다.\")\n",
    "\n",
    "    for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "        # 현재 에폭에 사용할 DataLoader 선택\n",
    "        dataloader_index = epoch % len(train_dataloader)  # 순환하면서 선택\n",
    "        current_dataloader = train_dataloader[dataloader_index]\n",
    "\n",
    "        print(f\"{epoch + 1}/{start_epoch + num_epochs} epoch start with Train Dataloader {dataloader_index + 1}\")\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_iou = [0.0] * num_classes\n",
    "        total_train_samples = 0\n",
    "\n",
    "        # 선택된 DataLoader만 사용하여 학습\n",
    "        for images, masks in tqdm(current_dataloader, desc=f\"Train Epoch {epoch + 1}, Dataloader {dataloader_index + 1}\", leave=False):\n",
    "            images = images.to(device)\n",
    "            masks = masks.long().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            outputs = torch.argmax(outputs, dim=1)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            total_train_samples += images.size(0)\n",
    "\n",
    "            # 클래스별 IoU 계산\n",
    "            class_iou = calculate_iou_per_class(outputs, masks, num_classes)\n",
    "            for j in range(num_classes):\n",
    "                if not np.isnan(class_iou[j]):\n",
    "                    train_iou[j] += class_iou[j]\n",
    "\n",
    "        # 평균 IoU 및 손실 계산\n",
    "        epoch_loss = running_loss / total_train_samples\n",
    "        train_iou = [x / len(current_dataloader) for x in train_iou]\n",
    "        train_miou = np.nanmean(train_iou[1:])  # 배경 클래스 제외\n",
    "\n",
    "        print(f\"{epoch + 1} epoch - Train Loss: {epoch_loss:.4f}, Train mIoU: {train_miou:.4f}\")\n",
    "        print(\"classes Train IoU:\", [f\"{CLASS_NAMES[i]}: {iou:.4f}\" for i, iou in enumerate(train_iou[1:], start=1)])\n",
    "\n",
    "        # 검증 단계는 매 에폭마다 수행\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_iou = [0.0] * num_classes\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_dataloader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.long().to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                outputs = torch.argmax(outputs, dim=1)\n",
    "                val_running_loss += loss.item() * images.size(0)\n",
    "\n",
    "                # 클래스별 IoU 계산\n",
    "                class_iou = calculate_iou_per_class(outputs, masks, num_classes)\n",
    "                for j in range(num_classes):\n",
    "                    if not np.isnan(class_iou[j]):\n",
    "                        val_iou[j] += class_iou[j]\n",
    "\n",
    "        # 검증 데이터에 대한 평균 IoU 및 손실 계산\n",
    "        val_loss = val_running_loss / len(val_dataloader.dataset)\n",
    "        val_iou = [x / len(val_dataloader) for x in val_iou]\n",
    "        val_miou = np.nanmean(val_iou[1:])  # 배경 클래스 제외\n",
    "\n",
    "        print(f\"{epoch + 1} epoch - Validation Loss: {val_loss:.4f}, Validation mIoU: {val_miou:.4f}\")\n",
    "        print(\"classes Validation IoU:\", [f\"{CLASS_NAMES[i]}: {iou:.4f}\" for i, iou in enumerate(val_iou[1:], start=1)])\n",
    "\n",
    "        # 성능 지표를 CSV에 저장\n",
    "        save_metrics_to_csv(epoch + 1, epoch_loss, val_loss, train_miou, val_miou, train_iou[1:], val_iou[1:])\n",
    "\n",
    "        # 최적의 검증 성능 기준으로 체크포인트 저장\n",
    "        if val_miou > best_val_miou:\n",
    "            best_val_miou = val_miou\n",
    "            best_checkpoint_filename = f\"best_checkpoint_epoch_{epoch + 1}.pth\"\n",
    "            save_checkpoint(epoch + 1, model, optimizer, checkpoint_dir, filename=best_checkpoint_filename)\n",
    "\n",
    "    # 마지막 에폭 체크포인트 저장\n",
    "    last_checkpoint_filename = f\"last_checkpoint_epoch_{epoch + 1}.pth\"\n",
    "    save_checkpoint(epoch + 1, model, optimizer, checkpoint_dir, filename=last_checkpoint_filename)\n",
    "    print(\"훈련 완료.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 장치: cuda:0\n",
      "체크포인트가 없거나 파일을 찾을 수 없습니다. 처음부터 학습을 시작합니다.\n",
      "1/100 epoch start with Train Dataloader 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch - Train Loss: 0.5718, Train mIoU: 0.3691\n",
      "classes Train IoU: ['Sidewalk: 0.6828', 'Braille Guide Blocks: 0.1533', 'Roadway: 0.2810', 'Alley: 0.6652', 'Bike Lane: 0.5045', 'Caution Zone: 0.1937', 'Cross walk: 0.1033']\n",
      "1 epoch - Validation Loss: 0.4565, Validation mIoU: 0.4461\n",
      "classes Validation IoU: ['Sidewalk: 0.6834', 'Braille Guide Blocks: 0.4840', 'Roadway: 0.2566', 'Alley: 0.7144', 'Bike Lane: 0.5048', 'Caution Zone: 0.2862', 'Cross walk: 0.1935']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 1 as ./resize_checkpoints/best_checkpoint_epoch_1.pth.\n",
      "2/100 epoch start with Train Dataloader 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 epoch - Train Loss: 0.3786, Train mIoU: 0.4624\n",
      "classes Train IoU: ['Sidewalk: 0.6994', 'Braille Guide Blocks: 0.4272', 'Roadway: 0.3528', 'Alley: 0.7279', 'Bike Lane: 0.5719', 'Caution Zone: 0.2824', 'Cross walk: 0.1750']\n",
      "2 epoch - Validation Loss: 0.3881, Validation mIoU: 0.4858\n",
      "classes Validation IoU: ['Sidewalk: 0.6586', 'Braille Guide Blocks: 0.5276', 'Roadway: 0.2914', 'Alley: 0.7411', 'Bike Lane: 0.5820', 'Caution Zone: 0.3308', 'Cross walk: 0.2690']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 2 as ./resize_checkpoints/best_checkpoint_epoch_2.pth.\n",
      "3/100 epoch start with Train Dataloader 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 epoch - Train Loss: 0.3787, Train mIoU: 0.4775\n",
      "classes Train IoU: ['Sidewalk: 0.6771', 'Braille Guide Blocks: 0.4856', 'Roadway: 0.3689', 'Alley: 0.7135', 'Bike Lane: 0.6075', 'Caution Zone: 0.3013', 'Cross walk: 0.1885']\n",
      "3 epoch - Validation Loss: 0.3696, Validation mIoU: 0.5105\n",
      "classes Validation IoU: ['Sidewalk: 0.6692', 'Braille Guide Blocks: 0.5881', 'Roadway: 0.3750', 'Alley: 0.7515', 'Bike Lane: 0.6067', 'Caution Zone: 0.3613', 'Cross walk: 0.2220']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 3 as ./resize_checkpoints/best_checkpoint_epoch_3.pth.\n",
      "4/100 epoch start with Train Dataloader 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 epoch - Train Loss: 0.3644, Train mIoU: 0.5015\n",
      "classes Train IoU: ['Sidewalk: 0.6826', 'Braille Guide Blocks: 0.5605', 'Roadway: 0.3808', 'Alley: 0.7418', 'Bike Lane: 0.5830', 'Caution Zone: 0.3389', 'Cross walk: 0.2227']\n",
      "4 epoch - Validation Loss: 0.3232, Validation mIoU: 0.5324\n",
      "classes Validation IoU: ['Sidewalk: 0.6864', 'Braille Guide Blocks: 0.6029', 'Roadway: 0.3266', 'Alley: 0.7521', 'Bike Lane: 0.6270', 'Caution Zone: 0.4469', 'Cross walk: 0.2848']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 4 as ./resize_checkpoints/best_checkpoint_epoch_4.pth.\n",
      "5/100 epoch start with Train Dataloader 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 epoch - Train Loss: 0.3443, Train mIoU: 0.5133\n",
      "classes Train IoU: ['Sidewalk: 0.6832', 'Braille Guide Blocks: 0.5187', 'Roadway: 0.4548', 'Alley: 0.7191', 'Bike Lane: 0.6684', 'Caution Zone: 0.3465', 'Cross walk: 0.2026']\n",
      "5 epoch - Validation Loss: 0.3757, Validation mIoU: 0.4875\n",
      "classes Validation IoU: ['Sidewalk: 0.6543', 'Braille Guide Blocks: 0.5578', 'Roadway: 0.3559', 'Alley: 0.7565', 'Bike Lane: 0.6177', 'Caution Zone: 0.2173', 'Cross walk: 0.2530']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "6/100 epoch start with Train Dataloader 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 epoch - Train Loss: 0.3214, Train mIoU: 0.5158\n",
      "classes Train IoU: ['Sidewalk: 0.6870', 'Braille Guide Blocks: 0.5181', 'Roadway: 0.4556', 'Alley: 0.7467', 'Bike Lane: 0.6153', 'Caution Zone: 0.3630', 'Cross walk: 0.2247']\n",
      "6 epoch - Validation Loss: 0.3520, Validation mIoU: 0.5123\n",
      "classes Validation IoU: ['Sidewalk: 0.6834', 'Braille Guide Blocks: 0.6292', 'Roadway: 0.3582', 'Alley: 0.7560', 'Bike Lane: 0.6307', 'Caution Zone: 0.3155', 'Cross walk: 0.2134']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "7/100 epoch start with Train Dataloader 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 epoch - Train Loss: 0.3547, Train mIoU: 0.5064\n",
      "classes Train IoU: ['Sidewalk: 0.6682', 'Braille Guide Blocks: 0.5397', 'Roadway: 0.4225', 'Alley: 0.7223', 'Bike Lane: 0.6231', 'Caution Zone: 0.3415', 'Cross walk: 0.2278']\n",
      "7 epoch - Validation Loss: 0.3478, Validation mIoU: 0.5040\n",
      "classes Validation IoU: ['Sidewalk: 0.6586', 'Braille Guide Blocks: 0.5404', 'Roadway: 0.3367', 'Alley: 0.7376', 'Bike Lane: 0.6500', 'Caution Zone: 0.2799', 'Cross walk: 0.3250']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "8/100 epoch start with Train Dataloader 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 epoch - Train Loss: 0.3482, Train mIoU: 0.5215\n",
      "classes Train IoU: ['Sidewalk: 0.6750', 'Braille Guide Blocks: 0.5672', 'Roadway: 0.4054', 'Alley: 0.7459', 'Bike Lane: 0.5972', 'Caution Zone: 0.3787', 'Cross walk: 0.2808']\n",
      "8 epoch - Validation Loss: 0.3301, Validation mIoU: 0.5320\n",
      "classes Validation IoU: ['Sidewalk: 0.6939', 'Braille Guide Blocks: 0.6019', 'Roadway: 0.3982', 'Alley: 0.7541', 'Bike Lane: 0.6216', 'Caution Zone: 0.3880', 'Cross walk: 0.2663']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "9/100 epoch start with Train Dataloader 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 epoch - Train Loss: 0.3355, Train mIoU: 0.5240\n",
      "classes Train IoU: ['Sidewalk: 0.6805', 'Braille Guide Blocks: 0.5441', 'Roadway: 0.4600', 'Alley: 0.7249', 'Bike Lane: 0.6348', 'Caution Zone: 0.3863', 'Cross walk: 0.2372']\n",
      "9 epoch - Validation Loss: 0.3614, Validation mIoU: 0.5107\n",
      "classes Validation IoU: ['Sidewalk: 0.6787', 'Braille Guide Blocks: 0.6302', 'Roadway: 0.3549', 'Alley: 0.7328', 'Bike Lane: 0.6255', 'Caution Zone: 0.2991', 'Cross walk: 0.2535']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "10/100 epoch start with Train Dataloader 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 epoch - Train Loss: 0.3115, Train mIoU: 0.5229\n",
      "classes Train IoU: ['Sidewalk: 0.6869', 'Braille Guide Blocks: 0.5342', 'Roadway: 0.4589', 'Alley: 0.7519', 'Bike Lane: 0.6366', 'Caution Zone: 0.3614', 'Cross walk: 0.2304']\n",
      "10 epoch - Validation Loss: 0.3674, Validation mIoU: 0.5074\n",
      "classes Validation IoU: ['Sidewalk: 0.6753', 'Braille Guide Blocks: 0.6205', 'Roadway: 0.3593', 'Alley: 0.7392', 'Bike Lane: 0.6356', 'Caution Zone: 0.2389', 'Cross walk: 0.2827']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "11/100 epoch start with Train Dataloader 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 epoch - Train Loss: 0.3486, Train mIoU: 0.5147\n",
      "classes Train IoU: ['Sidewalk: 0.6735', 'Braille Guide Blocks: 0.5370', 'Roadway: 0.4216', 'Alley: 0.7263', 'Bike Lane: 0.6522', 'Caution Zone: 0.3545', 'Cross walk: 0.2376']\n",
      "11 epoch - Validation Loss: 0.3615, Validation mIoU: 0.5294\n",
      "classes Validation IoU: ['Sidewalk: 0.6617', 'Braille Guide Blocks: 0.5929', 'Roadway: 0.3582', 'Alley: 0.7249', 'Bike Lane: 0.6401', 'Caution Zone: 0.4244', 'Cross walk: 0.3033']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "12/100 epoch start with Train Dataloader 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 epoch - Train Loss: 0.3410, Train mIoU: 0.5192\n",
      "classes Train IoU: ['Sidewalk: 0.6750', 'Braille Guide Blocks: 0.5978', 'Roadway: 0.3992', 'Alley: 0.7458', 'Bike Lane: 0.5866', 'Caution Zone: 0.3442', 'Cross walk: 0.2858']\n",
      "12 epoch - Validation Loss: 0.3353, Validation mIoU: 0.5303\n",
      "classes Validation IoU: ['Sidewalk: 0.6977', 'Braille Guide Blocks: 0.6059', 'Roadway: 0.4069', 'Alley: 0.7704', 'Bike Lane: 0.6461', 'Caution Zone: 0.4029', 'Cross walk: 0.1820']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "13/100 epoch start with Train Dataloader 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 epoch - Train Loss: 0.3273, Train mIoU: 0.5327\n",
      "classes Train IoU: ['Sidewalk: 0.6696', 'Braille Guide Blocks: 0.5372', 'Roadway: 0.4842', 'Alley: 0.7330', 'Bike Lane: 0.6669', 'Caution Zone: 0.3875', 'Cross walk: 0.2502']\n",
      "13 epoch - Validation Loss: 0.4121, Validation mIoU: 0.4829\n",
      "classes Validation IoU: ['Sidewalk: 0.6489', 'Braille Guide Blocks: 0.5846', 'Roadway: 0.3692', 'Alley: 0.7065', 'Bike Lane: 0.4595', 'Caution Zone: 0.2986', 'Cross walk: 0.3127']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "14/100 epoch start with Train Dataloader 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 epoch - Train Loss: 0.3140, Train mIoU: 0.5207\n",
      "classes Train IoU: ['Sidewalk: 0.6820', 'Braille Guide Blocks: 0.5515', 'Roadway: 0.4583', 'Alley: 0.7527', 'Bike Lane: 0.6145', 'Caution Zone: 0.3499', 'Cross walk: 0.2361']\n",
      "14 epoch - Validation Loss: 0.3777, Validation mIoU: 0.5260\n",
      "classes Validation IoU: ['Sidewalk: 0.6534', 'Braille Guide Blocks: 0.6260', 'Roadway: 0.3995', 'Alley: 0.7356', 'Bike Lane: 0.6480', 'Caution Zone: 0.3687', 'Cross walk: 0.2508']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "15/100 epoch start with Train Dataloader 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 epoch - Train Loss: 0.3398, Train mIoU: 0.5230\n",
      "classes Train IoU: ['Sidewalk: 0.6684', 'Braille Guide Blocks: 0.5603', 'Roadway: 0.4421', 'Alley: 0.7354', 'Bike Lane: 0.6512', 'Caution Zone: 0.3600', 'Cross walk: 0.2437']\n",
      "15 epoch - Validation Loss: 0.3447, Validation mIoU: 0.5198\n",
      "classes Validation IoU: ['Sidewalk: 0.6701', 'Braille Guide Blocks: 0.6049', 'Roadway: 0.4057', 'Alley: 0.7321', 'Bike Lane: 0.6174', 'Caution Zone: 0.3416', 'Cross walk: 0.2668']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "16/100 epoch start with Train Dataloader 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 epoch - Train Loss: 0.3341, Train mIoU: 0.5307\n",
      "classes Train IoU: ['Sidewalk: 0.6755', 'Braille Guide Blocks: 0.5901', 'Roadway: 0.4141', 'Alley: 0.7519', 'Bike Lane: 0.5976', 'Caution Zone: 0.4076', 'Cross walk: 0.2781']\n",
      "16 epoch - Validation Loss: 0.3300, Validation mIoU: 0.5390\n",
      "classes Validation IoU: ['Sidewalk: 0.6662', 'Braille Guide Blocks: 0.6285', 'Roadway: 0.3574', 'Alley: 0.7592', 'Bike Lane: 0.6492', 'Caution Zone: 0.4062', 'Cross walk: 0.3059']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 16 as ./resize_checkpoints/best_checkpoint_epoch_16.pth.\n",
      "17/100 epoch start with Train Dataloader 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 epoch - Train Loss: 0.3268, Train mIoU: 0.5409\n",
      "classes Train IoU: ['Sidewalk: 0.6670', 'Braille Guide Blocks: 0.5402', 'Roadway: 0.4774', 'Alley: 0.7342', 'Bike Lane: 0.6846', 'Caution Zone: 0.4137', 'Cross walk: 0.2690']\n",
      "17 epoch - Validation Loss: 0.3500, Validation mIoU: 0.5282\n",
      "classes Validation IoU: ['Sidewalk: 0.6601', 'Braille Guide Blocks: 0.6389', 'Roadway: 0.3838', 'Alley: 0.7524', 'Bike Lane: 0.6176', 'Caution Zone: 0.3290', 'Cross walk: 0.3154']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "18/100 epoch start with Train Dataloader 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 epoch - Train Loss: 0.3069, Train mIoU: 0.5292\n",
      "classes Train IoU: ['Sidewalk: 0.6826', 'Braille Guide Blocks: 0.5539', 'Roadway: 0.4601', 'Alley: 0.7564', 'Bike Lane: 0.6002', 'Caution Zone: 0.3970', 'Cross walk: 0.2545']\n",
      "18 epoch - Validation Loss: 0.3495, Validation mIoU: 0.5349\n",
      "classes Validation IoU: ['Sidewalk: 0.6605', 'Braille Guide Blocks: 0.6493', 'Roadway: 0.3948', 'Alley: 0.7581', 'Bike Lane: 0.5336', 'Caution Zone: 0.4158', 'Cross walk: 0.3323']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "19/100 epoch start with Train Dataloader 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 epoch - Train Loss: 0.3401, Train mIoU: 0.5178\n",
      "classes Train IoU: ['Sidewalk: 0.6667', 'Braille Guide Blocks: 0.5560', 'Roadway: 0.4415', 'Alley: 0.7277', 'Bike Lane: 0.6440', 'Caution Zone: 0.3492', 'Cross walk: 0.2393']\n",
      "19 epoch - Validation Loss: 0.3543, Validation mIoU: 0.5224\n",
      "classes Validation IoU: ['Sidewalk: 0.6651', 'Braille Guide Blocks: 0.6302', 'Roadway: 0.3978', 'Alley: 0.7414', 'Bike Lane: 0.4833', 'Caution Zone: 0.4302', 'Cross walk: 0.3089']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "20/100 epoch start with Train Dataloader 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 epoch - Train Loss: 0.3283, Train mIoU: 0.5299\n",
      "classes Train IoU: ['Sidewalk: 0.6788', 'Braille Guide Blocks: 0.6000', 'Roadway: 0.4257', 'Alley: 0.7499', 'Bike Lane: 0.6039', 'Caution Zone: 0.3933', 'Cross walk: 0.2577']\n",
      "20 epoch - Validation Loss: 0.3189, Validation mIoU: 0.5334\n",
      "classes Validation IoU: ['Sidewalk: 0.6850', 'Braille Guide Blocks: 0.6163', 'Roadway: 0.3921', 'Alley: 0.7597', 'Bike Lane: 0.6313', 'Caution Zone: 0.3954', 'Cross walk: 0.2537']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "21/100 epoch start with Train Dataloader 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 epoch - Train Loss: 0.3267, Train mIoU: 0.5294\n",
      "classes Train IoU: ['Sidewalk: 0.6695', 'Braille Guide Blocks: 0.5273', 'Roadway: 0.4928', 'Alley: 0.7293', 'Bike Lane: 0.6601', 'Caution Zone: 0.3940', 'Cross walk: 0.2328']\n",
      "21 epoch - Validation Loss: 0.3540, Validation mIoU: 0.5246\n",
      "classes Validation IoU: ['Sidewalk: 0.6625', 'Braille Guide Blocks: 0.6237', 'Roadway: 0.3695', 'Alley: 0.7426', 'Bike Lane: 0.6172', 'Caution Zone: 0.3822', 'Cross walk: 0.2745']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "22/100 epoch start with Train Dataloader 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 epoch - Train Loss: 0.3026, Train mIoU: 0.5305\n",
      "classes Train IoU: ['Sidewalk: 0.6821', 'Braille Guide Blocks: 0.5601', 'Roadway: 0.4708', 'Alley: 0.7631', 'Bike Lane: 0.6077', 'Caution Zone: 0.3823', 'Cross walk: 0.2476']\n",
      "22 epoch - Validation Loss: 0.3629, Validation mIoU: 0.5336\n",
      "classes Validation IoU: ['Sidewalk: 0.6806', 'Braille Guide Blocks: 0.6508', 'Roadway: 0.3287', 'Alley: 0.7447', 'Bike Lane: 0.6495', 'Caution Zone: 0.4122', 'Cross walk: 0.2689']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "23/100 epoch start with Train Dataloader 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 epoch - Train Loss: 0.3343, Train mIoU: 0.5258\n",
      "classes Train IoU: ['Sidewalk: 0.6718', 'Braille Guide Blocks: 0.5494', 'Roadway: 0.4405', 'Alley: 0.7338', 'Bike Lane: 0.6752', 'Caution Zone: 0.3645', 'Cross walk: 0.2455']\n",
      "23 epoch - Validation Loss: 0.3368, Validation mIoU: 0.5187\n",
      "classes Validation IoU: ['Sidewalk: 0.6700', 'Braille Guide Blocks: 0.5919', 'Roadway: 0.3433', 'Alley: 0.7520', 'Bike Lane: 0.6453', 'Caution Zone: 0.3292', 'Cross walk: 0.2989']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "24/100 epoch start with Train Dataloader 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 epoch - Train Loss: 0.3219, Train mIoU: 0.5403\n",
      "classes Train IoU: ['Sidewalk: 0.6833', 'Braille Guide Blocks: 0.6127', 'Roadway: 0.4106', 'Alley: 0.7556', 'Bike Lane: 0.6440', 'Caution Zone: 0.3969', 'Cross walk: 0.2794']\n",
      "24 epoch - Validation Loss: 0.3180, Validation mIoU: 0.5468\n",
      "classes Validation IoU: ['Sidewalk: 0.6785', 'Braille Guide Blocks: 0.6223', 'Roadway: 0.3989', 'Alley: 0.7730', 'Bike Lane: 0.6393', 'Caution Zone: 0.3939', 'Cross walk: 0.3214']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 24 as ./resize_checkpoints/best_checkpoint_epoch_24.pth.\n",
      "25/100 epoch start with Train Dataloader 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 epoch - Train Loss: 0.3134, Train mIoU: 0.5448\n",
      "classes Train IoU: ['Sidewalk: 0.6720', 'Braille Guide Blocks: 0.5485', 'Roadway: 0.4975', 'Alley: 0.7402', 'Bike Lane: 0.6806', 'Caution Zone: 0.4279', 'Cross walk: 0.2470']\n",
      "25 epoch - Validation Loss: 0.3558, Validation mIoU: 0.5363\n",
      "classes Validation IoU: ['Sidewalk: 0.6832', 'Braille Guide Blocks: 0.6266', 'Roadway: 0.3833', 'Alley: 0.7172', 'Bike Lane: 0.6317', 'Caution Zone: 0.4121', 'Cross walk: 0.3000']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "26/100 epoch start with Train Dataloader 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 epoch - Train Loss: 0.2904, Train mIoU: 0.5406\n",
      "classes Train IoU: ['Sidewalk: 0.6894', 'Braille Guide Blocks: 0.5518', 'Roadway: 0.4653', 'Alley: 0.7621', 'Bike Lane: 0.6594', 'Caution Zone: 0.4042', 'Cross walk: 0.2523']\n",
      "26 epoch - Validation Loss: 0.3288, Validation mIoU: 0.5328\n",
      "classes Validation IoU: ['Sidewalk: 0.6629', 'Braille Guide Blocks: 0.6184', 'Roadway: 0.3860', 'Alley: 0.7707', 'Bike Lane: 0.6062', 'Caution Zone: 0.3729', 'Cross walk: 0.3127']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "27/100 epoch start with Train Dataloader 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 epoch - Train Loss: 0.3276, Train mIoU: 0.5290\n",
      "classes Train IoU: ['Sidewalk: 0.6702', 'Braille Guide Blocks: 0.5623', 'Roadway: 0.4385', 'Alley: 0.7401', 'Bike Lane: 0.6807', 'Caution Zone: 0.3592', 'Cross walk: 0.2520']\n",
      "27 epoch - Validation Loss: 0.3865, Validation mIoU: 0.5016\n",
      "classes Validation IoU: ['Sidewalk: 0.6385', 'Braille Guide Blocks: 0.6285', 'Roadway: 0.3717', 'Alley: 0.7056', 'Bike Lane: 0.6513', 'Caution Zone: 0.1899', 'Cross walk: 0.3257']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "28/100 epoch start with Train Dataloader 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 epoch - Train Loss: 0.3199, Train mIoU: 0.5413\n",
      "classes Train IoU: ['Sidewalk: 0.6864', 'Braille Guide Blocks: 0.6176', 'Roadway: 0.4117', 'Alley: 0.7556', 'Bike Lane: 0.6432', 'Caution Zone: 0.3911', 'Cross walk: 0.2834']\n",
      "28 epoch - Validation Loss: 0.3415, Validation mIoU: 0.5086\n",
      "classes Validation IoU: ['Sidewalk: 0.6876', 'Braille Guide Blocks: 0.5892', 'Roadway: 0.3414', 'Alley: 0.7454', 'Bike Lane: 0.6405', 'Caution Zone: 0.3311', 'Cross walk: 0.2251']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "29/100 epoch start with Train Dataloader 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 epoch - Train Loss: 0.3158, Train mIoU: 0.5414\n",
      "classes Train IoU: ['Sidewalk: 0.6745', 'Braille Guide Blocks: 0.5611', 'Roadway: 0.4850', 'Alley: 0.7395', 'Bike Lane: 0.6707', 'Caution Zone: 0.4200', 'Cross walk: 0.2388']\n",
      "29 epoch - Validation Loss: 0.3569, Validation mIoU: 0.5346\n",
      "classes Validation IoU: ['Sidewalk: 0.6658', 'Braille Guide Blocks: 0.6278', 'Roadway: 0.3722', 'Alley: 0.7491', 'Bike Lane: 0.5916', 'Caution Zone: 0.4093', 'Cross walk: 0.3262']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "30/100 epoch start with Train Dataloader 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 epoch - Train Loss: 0.2841, Train mIoU: 0.5456\n",
      "classes Train IoU: ['Sidewalk: 0.6877', 'Braille Guide Blocks: 0.5606', 'Roadway: 0.4657', 'Alley: 0.7671', 'Bike Lane: 0.6550', 'Caution Zone: 0.4283', 'Cross walk: 0.2545']\n",
      "30 epoch - Validation Loss: 0.3463, Validation mIoU: 0.5178\n",
      "classes Validation IoU: ['Sidewalk: 0.6537', 'Braille Guide Blocks: 0.5918', 'Roadway: 0.3975', 'Alley: 0.7569', 'Bike Lane: 0.5796', 'Caution Zone: 0.3835', 'Cross walk: 0.2615']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "31/100 epoch start with Train Dataloader 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 epoch - Train Loss: 0.3287, Train mIoU: 0.5309\n",
      "classes Train IoU: ['Sidewalk: 0.6745', 'Braille Guide Blocks: 0.5706', 'Roadway: 0.4278', 'Alley: 0.7359', 'Bike Lane: 0.6644', 'Caution Zone: 0.3865', 'Cross walk: 0.2564']\n",
      "31 epoch - Validation Loss: 0.3540, Validation mIoU: 0.5020\n",
      "classes Validation IoU: ['Sidewalk: 0.6911', 'Braille Guide Blocks: 0.6456', 'Roadway: 0.3768', 'Alley: 0.7601', 'Bike Lane: 0.6205', 'Caution Zone: 0.2493', 'Cross walk: 0.1708']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "32/100 epoch start with Train Dataloader 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 epoch - Train Loss: 0.3152, Train mIoU: 0.5398\n",
      "classes Train IoU: ['Sidewalk: 0.6894', 'Braille Guide Blocks: 0.6035', 'Roadway: 0.4298', 'Alley: 0.7597', 'Bike Lane: 0.6491', 'Caution Zone: 0.3703', 'Cross walk: 0.2769']\n",
      "32 epoch - Validation Loss: 0.3009, Validation mIoU: 0.5541\n",
      "classes Validation IoU: ['Sidewalk: 0.6801', 'Braille Guide Blocks: 0.6400', 'Roadway: 0.3720', 'Alley: 0.7657', 'Bike Lane: 0.6301', 'Caution Zone: 0.4645', 'Cross walk: 0.3262']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 32 as ./resize_checkpoints/best_checkpoint_epoch_32.pth.\n",
      "33/100 epoch start with Train Dataloader 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 epoch - Train Loss: 0.3047, Train mIoU: 0.5406\n",
      "classes Train IoU: ['Sidewalk: 0.6782', 'Braille Guide Blocks: 0.5315', 'Roadway: 0.4875', 'Alley: 0.7337', 'Bike Lane: 0.6836', 'Caution Zone: 0.4167', 'Cross walk: 0.2529']\n",
      "33 epoch - Validation Loss: 0.3349, Validation mIoU: 0.5387\n",
      "classes Validation IoU: ['Sidewalk: 0.6571', 'Braille Guide Blocks: 0.6418', 'Roadway: 0.3913', 'Alley: 0.7581', 'Bike Lane: 0.6481', 'Caution Zone: 0.3142', 'Cross walk: 0.3602']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "34/100 epoch start with Train Dataloader 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 epoch - Train Loss: 0.2832, Train mIoU: 0.5460\n",
      "classes Train IoU: ['Sidewalk: 0.6881', 'Braille Guide Blocks: 0.5508', 'Roadway: 0.4685', 'Alley: 0.7658', 'Bike Lane: 0.6788', 'Caution Zone: 0.4078', 'Cross walk: 0.2622']\n",
      "34 epoch - Validation Loss: 0.3390, Validation mIoU: 0.5363\n",
      "classes Validation IoU: ['Sidewalk: 0.6758', 'Braille Guide Blocks: 0.6501', 'Roadway: 0.3698', 'Alley: 0.7407', 'Bike Lane: 0.6476', 'Caution Zone: 0.3873', 'Cross walk: 0.2825']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "35/100 epoch start with Train Dataloader 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 epoch - Train Loss: 0.3174, Train mIoU: 0.5317\n",
      "classes Train IoU: ['Sidewalk: 0.6777', 'Braille Guide Blocks: 0.5633', 'Roadway: 0.4316', 'Alley: 0.7400', 'Bike Lane: 0.6775', 'Caution Zone: 0.3697', 'Cross walk: 0.2623']\n",
      "35 epoch - Validation Loss: 0.3297, Validation mIoU: 0.5426\n",
      "classes Validation IoU: ['Sidewalk: 0.6571', 'Braille Guide Blocks: 0.6267', 'Roadway: 0.4086', 'Alley: 0.7664', 'Bike Lane: 0.6534', 'Caution Zone: 0.3565', 'Cross walk: 0.3295']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "36/100 epoch start with Train Dataloader 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 epoch - Train Loss: 0.3070, Train mIoU: 0.5470\n",
      "classes Train IoU: ['Sidewalk: 0.6895', 'Braille Guide Blocks: 0.5971', 'Roadway: 0.4311', 'Alley: 0.7619', 'Bike Lane: 0.6585', 'Caution Zone: 0.4105', 'Cross walk: 0.2806']\n",
      "36 epoch - Validation Loss: 0.3053, Validation mIoU: 0.5472\n",
      "classes Validation IoU: ['Sidewalk: 0.6781', 'Braille Guide Blocks: 0.6086', 'Roadway: 0.3987', 'Alley: 0.7750', 'Bike Lane: 0.6432', 'Caution Zone: 0.4268', 'Cross walk: 0.2998']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "37/100 epoch start with Train Dataloader 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 epoch - Train Loss: 0.2972, Train mIoU: 0.5486\n",
      "classes Train IoU: ['Sidewalk: 0.6807', 'Braille Guide Blocks: 0.5471', 'Roadway: 0.5004', 'Alley: 0.7432', 'Bike Lane: 0.6869', 'Caution Zone: 0.4246', 'Cross walk: 0.2574']\n",
      "37 epoch - Validation Loss: 0.3582, Validation mIoU: 0.5229\n",
      "classes Validation IoU: ['Sidewalk: 0.6729', 'Braille Guide Blocks: 0.6058', 'Roadway: 0.3753', 'Alley: 0.7609', 'Bike Lane: 0.6399', 'Caution Zone: 0.3886', 'Cross walk: 0.2169']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "38/100 epoch start with Train Dataloader 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 epoch - Train Loss: 0.2794, Train mIoU: 0.5447\n",
      "classes Train IoU: ['Sidewalk: 0.6874', 'Braille Guide Blocks: 0.5669', 'Roadway: 0.4676', 'Alley: 0.7678', 'Bike Lane: 0.6392', 'Caution Zone: 0.4218', 'Cross walk: 0.2621']\n",
      "38 epoch - Validation Loss: 0.3174, Validation mIoU: 0.5470\n",
      "classes Validation IoU: ['Sidewalk: 0.6846', 'Braille Guide Blocks: 0.6232', 'Roadway: 0.3848', 'Alley: 0.7505', 'Bike Lane: 0.6424', 'Caution Zone: 0.4423', 'Cross walk: 0.3012']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "39/100 epoch start with Train Dataloader 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 epoch - Train Loss: 0.3087, Train mIoU: 0.5407\n",
      "classes Train IoU: ['Sidewalk: 0.6786', 'Braille Guide Blocks: 0.5820', 'Roadway: 0.4428', 'Alley: 0.7444', 'Bike Lane: 0.6846', 'Caution Zone: 0.4024', 'Cross walk: 0.2499']\n",
      "39 epoch - Validation Loss: 0.3238, Validation mIoU: 0.5456\n",
      "classes Validation IoU: ['Sidewalk: 0.6599', 'Braille Guide Blocks: 0.6434', 'Roadway: 0.3729', 'Alley: 0.7614', 'Bike Lane: 0.6434', 'Caution Zone: 0.4337', 'Cross walk: 0.3048']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "40/100 epoch start with Train Dataloader 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 epoch - Train Loss: 0.3013, Train mIoU: 0.5531\n",
      "classes Train IoU: ['Sidewalk: 0.6850', 'Braille Guide Blocks: 0.6223', 'Roadway: 0.4351', 'Alley: 0.7648', 'Bike Lane: 0.6499', 'Caution Zone: 0.4349', 'Cross walk: 0.2800']\n",
      "40 epoch - Validation Loss: 0.3155, Validation mIoU: 0.5429\n",
      "classes Validation IoU: ['Sidewalk: 0.6821', 'Braille Guide Blocks: 0.6419', 'Roadway: 0.3839', 'Alley: 0.7604', 'Bike Lane: 0.5842', 'Caution Zone: 0.4713', 'Cross walk: 0.2768']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "41/100 epoch start with Train Dataloader 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 epoch - Train Loss: 0.2969, Train mIoU: 0.5494\n",
      "classes Train IoU: ['Sidewalk: 0.6793', 'Braille Guide Blocks: 0.5632', 'Roadway: 0.4982', 'Alley: 0.7467', 'Bike Lane: 0.6773', 'Caution Zone: 0.4361', 'Cross walk: 0.2448']\n",
      "41 epoch - Validation Loss: 0.3572, Validation mIoU: 0.5290\n",
      "classes Validation IoU: ['Sidewalk: 0.6741', 'Braille Guide Blocks: 0.6563', 'Roadway: 0.3900', 'Alley: 0.7498', 'Bike Lane: 0.6202', 'Caution Zone: 0.3430', 'Cross walk: 0.2694']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "42/100 epoch start with Train Dataloader 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 epoch - Train Loss: 0.2704, Train mIoU: 0.5559\n",
      "classes Train IoU: ['Sidewalk: 0.6902', 'Braille Guide Blocks: 0.5810', 'Roadway: 0.4758', 'Alley: 0.7742', 'Bike Lane: 0.6653', 'Caution Zone: 0.4485', 'Cross walk: 0.2565']\n",
      "42 epoch - Validation Loss: 0.3238, Validation mIoU: 0.5486\n",
      "classes Validation IoU: ['Sidewalk: 0.6709', 'Braille Guide Blocks: 0.6388', 'Roadway: 0.3864', 'Alley: 0.7551', 'Bike Lane: 0.6420', 'Caution Zone: 0.4189', 'Cross walk: 0.3280']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "43/100 epoch start with Train Dataloader 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 epoch - Train Loss: 0.3102, Train mIoU: 0.5340\n",
      "classes Train IoU: ['Sidewalk: 0.6763', 'Braille Guide Blocks: 0.5536', 'Roadway: 0.4398', 'Alley: 0.7455', 'Bike Lane: 0.6728', 'Caution Zone: 0.3909', 'Cross walk: 0.2593']\n",
      "43 epoch - Validation Loss: 0.3240, Validation mIoU: 0.5514\n",
      "classes Validation IoU: ['Sidewalk: 0.6678', 'Braille Guide Blocks: 0.6519', 'Roadway: 0.3780', 'Alley: 0.7526', 'Bike Lane: 0.6335', 'Caution Zone: 0.4221', 'Cross walk: 0.3539']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "44/100 epoch start with Train Dataloader 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 epoch - Train Loss: 0.3030, Train mIoU: 0.5440\n",
      "classes Train IoU: ['Sidewalk: 0.6873', 'Braille Guide Blocks: 0.6019', 'Roadway: 0.4198', 'Alley: 0.7616', 'Bike Lane: 0.6296', 'Caution Zone: 0.4296', 'Cross walk: 0.2779']\n",
      "44 epoch - Validation Loss: 0.3345, Validation mIoU: 0.5309\n",
      "classes Validation IoU: ['Sidewalk: 0.6826', 'Braille Guide Blocks: 0.6280', 'Roadway: 0.3619', 'Alley: 0.7647', 'Bike Lane: 0.5805', 'Caution Zone: 0.4677', 'Cross walk: 0.2305']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "45/100 epoch start with Train Dataloader 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 epoch - Train Loss: 0.2861, Train mIoU: 0.5514\n",
      "classes Train IoU: ['Sidewalk: 0.6851', 'Braille Guide Blocks: 0.5665', 'Roadway: 0.4902', 'Alley: 0.7550', 'Bike Lane: 0.6872', 'Caution Zone: 0.4470', 'Cross walk: 0.2287']\n",
      "45 epoch - Validation Loss: 0.3662, Validation mIoU: 0.5162\n",
      "classes Validation IoU: ['Sidewalk: 0.6761', 'Braille Guide Blocks: 0.6090', 'Roadway: 0.3397', 'Alley: 0.7469', 'Bike Lane: 0.6539', 'Caution Zone: 0.3070', 'Cross walk: 0.2807']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "46/100 epoch start with Train Dataloader 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 epoch - Train Loss: 0.2678, Train mIoU: 0.5544\n",
      "classes Train IoU: ['Sidewalk: 0.6946', 'Braille Guide Blocks: 0.5764', 'Roadway: 0.4644', 'Alley: 0.7701', 'Bike Lane: 0.6665', 'Caution Zone: 0.4565', 'Cross walk: 0.2521']\n",
      "46 epoch - Validation Loss: 0.3334, Validation mIoU: 0.5489\n",
      "classes Validation IoU: ['Sidewalk: 0.6765', 'Braille Guide Blocks: 0.6638', 'Roadway: 0.3842', 'Alley: 0.7416', 'Bike Lane: 0.6378', 'Caution Zone: 0.3936', 'Cross walk: 0.3448']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "47/100 epoch start with Train Dataloader 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 epoch - Train Loss: 0.3063, Train mIoU: 0.5380\n",
      "classes Train IoU: ['Sidewalk: 0.6764', 'Braille Guide Blocks: 0.5839', 'Roadway: 0.4430', 'Alley: 0.7427', 'Bike Lane: 0.6602', 'Caution Zone: 0.4063', 'Cross walk: 0.2538']\n",
      "47 epoch - Validation Loss: 0.3448, Validation mIoU: 0.5116\n",
      "classes Validation IoU: ['Sidewalk: 0.6696', 'Braille Guide Blocks: 0.6248', 'Roadway: 0.3812', 'Alley: 0.7674', 'Bike Lane: 0.6108', 'Caution Zone: 0.2592', 'Cross walk: 0.2682']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "48/100 epoch start with Train Dataloader 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 epoch - Train Loss: 0.2961, Train mIoU: 0.5494\n",
      "classes Train IoU: ['Sidewalk: 0.6900', 'Braille Guide Blocks: 0.6104', 'Roadway: 0.4366', 'Alley: 0.7680', 'Bike Lane: 0.6453', 'Caution Zone: 0.4336', 'Cross walk: 0.2620']\n",
      "48 epoch - Validation Loss: 0.3021, Validation mIoU: 0.5493\n",
      "classes Validation IoU: ['Sidewalk: 0.6889', 'Braille Guide Blocks: 0.6620', 'Roadway: 0.3699', 'Alley: 0.7668', 'Bike Lane: 0.6463', 'Caution Zone: 0.4186', 'Cross walk: 0.2927']\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "49/100 epoch start with Train Dataloader 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m checkpoint_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# 학습 함수 호출\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mtrain_model_with_csv_and_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcsv_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./resize_metrics.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./resize_checkpoints\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_file\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 여기에 원하는 체크포인트 파일을 지정\u001b[39;49;00m\n\u001b[1;32m     38\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 94\u001b[0m, in \u001b[0;36mtrain_model_with_csv_and_checkpoint\u001b[0;34m(model, train_dataloader, val_dataloader, criterion, optimizer, num_classes, num_epochs, device, csv_filename, checkpoint_dir, checkpoint_file)\u001b[0m\n\u001b[1;32m     91\u001b[0m total_train_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# 선택된 DataLoader만 사용하여 학습\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, masks \u001b[38;5;129;01min\u001b[39;00m tqdm(current_dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Dataloader \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataloader_index\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     95\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     96\u001b[0m     masks \u001b[38;5;241m=\u001b[39m masks\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[27], line 23\u001b[0m, in \u001b[0;36mCustomSegmentationDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     21\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_paths[idx]\n\u001b[1;32m     22\u001b[0m mask_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_paths[idx]\n\u001b[0;32m---> 23\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m mask \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(mask_path)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 이미지와 마스크에 동일한 변환 적용\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/PIL/Image.py:1007\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGBA\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mor\u001b[39;00m (mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m matrix):\n\u001b[0;32m-> 1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m matrix:\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;66;03m# matrix conversion\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/PIL/Image.py:1274\u001b[0m, in \u001b[0;36mImage.copy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;124;03mCopies this image. Use this method if you wish to paste things\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;124;03minto an image, but still retain the original.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;124;03m:returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m   1272\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1273\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m-> 1274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# CUDA 환경 설정\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "# 모델 및 학습 설정\n",
    "num_classes = 8  # 실제 학습할 클래스 수 (배경 제외)\n",
    "learning_rate = 0.0007  # 논문에 나와있는 learning rate\n",
    "weight_decay = 0.0005\n",
    "num_epochs = 100\n",
    "\n",
    "\n",
    "# 모델, 옵티마이저, 손실 함수 설정\n",
    "model = LightweightDeepLabV3(num_classes=num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# 손실 함수 및 장치 설정 (가중치 없이)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0).to(device)\n",
    "\n",
    "# 체크포인트 파일 지정 \n",
    "checkpoint_file = \"./\"\n",
    "\n",
    "# 학습 함수 호출\n",
    "train_model_with_csv_and_checkpoint(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloaders,\n",
    "    val_dataloader=val_dataloader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_classes=num_classes,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    "    csv_filename=\"./resize_metrics.csv\",\n",
    "    checkpoint_dir=\"./resize_checkpoints\",\n",
    "    checkpoint_file=checkpoint_file  # 여기에 원하는 체크포인트 파일을 지정\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Class names definition\n",
    "CLASS_NAMES = [\"Background\", \"Sidewalk\", \"Braille Guide Blocks\", \"Roadway\", \"Alley\", \"Bike Lane\", \"Caution Zone\", \"Cross walk\"]\n",
    "\n",
    "# 성능 지표를 CSV 파일에 저장하는 함수 (클래스별 mIoU 추가, 배경 클래스 제외)\n",
    "def save_metrics_to_csv(epoch, train_loss, val_loss, train_miou, val_miou, class_train_iou, class_val_iou, filename=\"resize_metrics.csv\"):\n",
    "    try:\n",
    "        file_exists = os.path.isfile(filename)\n",
    "        \n",
    "        with open(filename, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            if not file_exists:\n",
    "                # CSV 헤더에 클래스별 mIoU 추가 (배경 제외)\n",
    "                header = [\"Epoch\", \"Train Loss\", \"Val Loss\", \"Train mIoU\", \"Val mIoU\"] + \\\n",
    "                         [f\"Train IoU - {name}\" for name in CLASS_NAMES[1:]] + \\\n",
    "                         [f\"Val IoU - {name}\" for name in CLASS_NAMES[1:]]\n",
    "                writer.writerow(header)\n",
    "            \n",
    "            # 배경 클래스를 제외한 클래스별 mIoU와 함께 데이터를 작성\n",
    "            row = [epoch, train_loss, val_loss, train_miou, val_miou] + \\\n",
    "                  class_train_iou + class_val_iou\n",
    "            writer.writerow(row)\n",
    "        print(f\"Metrics saved successfully to {filename}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save metrics to CSV: {e}\")\n",
    "\n",
    "# Function to save model checkpoints\n",
    "def save_checkpoint(epoch, model, optimizer, checkpoint_dir, filename=\"last_checkpoint.pth\"):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "    checkpoint_filename = os.path.join(checkpoint_dir, filename)\n",
    "    torch.save(checkpoint, checkpoint_filename)\n",
    "    print(f\"Checkpoint saved at epoch {epoch} as {checkpoint_filename}.\")\n",
    "\n",
    "# Function to load model checkpoints\n",
    "def load_checkpoint(filename, model, optimizer=None):\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    print(f\"Checkpoint loaded from epoch {epoch}.\")\n",
    "    return epoch\n",
    "\n",
    "# Function to calculate IoU per class\n",
    "def calculate_iou_per_class(pred, target, num_classes):\n",
    "    iou_per_class = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = (pred_inds & target_inds).sum().float().item()\n",
    "        union = (pred_inds | target_inds).sum().float().item()\n",
    "        if union == 0:\n",
    "            iou_per_class.append(float('nan'))\n",
    "        else:\n",
    "            iou_per_class.append(intersection / union)\n",
    "    return iou_per_class\n",
    "\n",
    "\n",
    "def train_model_with_csv_and_checkpoint(model, train_dataloader, val_dataloader, criterion, optimizer, num_classes, num_epochs=25, device='cuda:0', csv_filename=\"resize_metrics.csv\", checkpoint_dir=\"./resize_checkpoints\", checkpoint_file=None):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    model.to(device)\n",
    "    print(\"모델 장치:\", next(model.parameters()).device)\n",
    "\n",
    "    best_val_miou = -1\n",
    "\n",
    "    # 체크포인트 파일을 불러오고, 이어서 학습할 시작 에폭 설정\n",
    "    start_epoch = 0\n",
    "    if checkpoint_file and os.path.isfile(checkpoint_file):\n",
    "        start_epoch = load_checkpoint(checkpoint_file, model, optimizer)\n",
    "        print(f\"체크포인트 로드 완료. {start_epoch + 1} 에폭부터 재개합니다.\")\n",
    "    else:\n",
    "        print(\"체크포인트가 없거나 파일을 찾을 수 없습니다. 처음부터 학습을 시작합니다.\")\n",
    "\n",
    "    # CSV에 기록할 에폭을 49부터 시작하도록 설정\n",
    "    csv_epoch = 49\n",
    "\n",
    "    # 학습 루프: `start_epoch + 1`부터 `start_epoch + num_epochs`까지\n",
    "    for epoch in range(start_epoch + 1, start_epoch + num_epochs + 1):\n",
    "        dataloader_index = epoch % len(train_dataloader)\n",
    "        current_dataloader = train_dataloader[dataloader_index]\n",
    "        \n",
    "        print(\"\\n=========================================================================\")\n",
    "        print(f\"{epoch}/{start_epoch + num_epochs} epoch start with Train Dataloader {dataloader_index + 1}\")\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_iou = [0.0] * num_classes\n",
    "        total_train_samples = 0\n",
    "\n",
    "        for images, masks in tqdm(current_dataloader, desc=f\"Train Epoch {epoch}, Dataloader {dataloader_index + 1}\", leave=False):\n",
    "            images = images.to(device)\n",
    "            masks = masks.long().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            outputs = torch.argmax(outputs, dim=1)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            total_train_samples += images.size(0)\n",
    "\n",
    "            class_iou = calculate_iou_per_class(outputs, masks, num_classes)\n",
    "            for j in range(num_classes):\n",
    "                if not np.isnan(class_iou[j]):\n",
    "                    train_iou[j] += class_iou[j]\n",
    "\n",
    "        epoch_loss = running_loss / total_train_samples\n",
    "        train_iou = [x / len(current_dataloader) for x in train_iou]\n",
    "        train_miou = np.nanmean(train_iou[1:])\n",
    "\n",
    "        print(f\"{epoch} epoch - Train Loss: {epoch_loss:.4f}, Train mIoU: {train_miou:.4f}\")\n",
    "\n",
    "        # 검증 단계 수행\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_iou = [0.0] * num_classes\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_dataloader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.long().to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                outputs = torch.argmax(outputs, dim=1)\n",
    "                val_running_loss += loss.item() * images.size(0)\n",
    "\n",
    "                class_iou = calculate_iou_per_class(outputs, masks, num_classes)\n",
    "                for j in range(num_classes):\n",
    "                    if not np.isnan(class_iou[j]):\n",
    "                        val_iou[j] += class_iou[j]\n",
    "\n",
    "        val_loss = val_running_loss / len(val_dataloader.dataset)\n",
    "        val_iou = [x / len(val_dataloader) for x in val_iou]\n",
    "        val_miou = np.nanmean(val_iou[1:])\n",
    "\n",
    "        print(f\"{epoch} epoch - Validation Loss: {val_loss:.4f}, Validation mIoU: {val_miou:.4f}\")\n",
    "\n",
    "        # 강제로 CSV에 기록할 에폭을 49부터 시작하도록 설정\n",
    "        save_metrics_to_csv(csv_epoch, epoch_loss, val_loss, train_miou, val_miou, train_iou[1:], val_iou[1:])\n",
    "        csv_epoch += 1  # CSV 에폭을 증가시켜 다음 에폭에 반영\n",
    "\n",
    "        # 매 에폭마다 체크포인트 저장\n",
    "        epoch_checkpoint_filename = f\"checkpoint_epoch_{epoch}.pth\"\n",
    "        save_checkpoint(epoch, model, optimizer, checkpoint_dir, filename=epoch_checkpoint_filename)\n",
    "        \n",
    "\n",
    "        # 최적의 검증 성능 기준으로 최상의 체크포인트도 업데이트\n",
    "        if val_miou > best_val_miou:\n",
    "            best_val_miou = val_miou\n",
    "            best_checkpoint_filename = f\"best_checkpoint_epoch_{epoch}.pth\"\n",
    "            save_checkpoint(epoch, model, optimizer, checkpoint_dir, filename=best_checkpoint_filename)\n",
    "\n",
    "    # 마지막 에폭 체크포인트 저장\n",
    "    last_checkpoint_filename = f\"last_checkpoint_epoch_{epoch}.pth\"\n",
    "    save_checkpoint(epoch, model, optimizer, checkpoint_dir, filename=last_checkpoint_filename)\n",
    "    print(\"훈련 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2775340/92660943.py:45: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 장치: cuda:0\n",
      "Checkpoint loaded from epoch 32.\n",
      "체크포인트 로드 완료. 33 에폭부터 재개합니다.\n",
      "33/132 epoch start with Train Dataloader 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 epoch - Train Loss: 0.2803, Train mIoU: 0.5474\n",
      "33 epoch - Validation Loss: 0.3404, Validation mIoU: 0.5275\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 33 as ./resize_checkpoints/checkpoint_epoch_33.pth.\n",
      "Checkpoint saved at epoch 33 as ./resize_checkpoints/best_checkpoint_epoch_33.pth.\n",
      "34/132 epoch start with Train Dataloader 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 epoch - Train Loss: 0.3082, Train mIoU: 0.5335\n",
      "34 epoch - Validation Loss: 0.3278, Validation mIoU: 0.5350\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 34 as ./resize_checkpoints/checkpoint_epoch_34.pth.\n",
      "Checkpoint saved at epoch 34 as ./resize_checkpoints/best_checkpoint_epoch_34.pth.\n",
      "35/132 epoch start with Train Dataloader 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 epoch - Train Loss: 0.3038, Train mIoU: 0.5508\n",
      "35 epoch - Validation Loss: 0.3076, Validation mIoU: 0.5413\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 35 as ./resize_checkpoints/checkpoint_epoch_35.pth.\n",
      "Checkpoint saved at epoch 35 as ./resize_checkpoints/best_checkpoint_epoch_35.pth.\n",
      "36/132 epoch start with Train Dataloader 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 epoch - Train Loss: 0.3098, Train mIoU: 0.5417\n",
      "36 epoch - Validation Loss: 0.3332, Validation mIoU: 0.5382\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 36 as ./resize_checkpoints/checkpoint_epoch_36.pth.\n",
      "37/132 epoch start with Train Dataloader 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 epoch - Train Loss: 0.2747, Train mIoU: 0.5494\n",
      "37 epoch - Validation Loss: 0.3436, Validation mIoU: 0.5447\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 37 as ./resize_checkpoints/checkpoint_epoch_37.pth.\n",
      "Checkpoint saved at epoch 37 as ./resize_checkpoints/best_checkpoint_epoch_37.pth.\n",
      "38/132 epoch start with Train Dataloader 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 epoch - Train Loss: 0.3118, Train mIoU: 0.5429\n",
      "38 epoch - Validation Loss: 0.3791, Validation mIoU: 0.5038\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 38 as ./resize_checkpoints/checkpoint_epoch_38.pth.\n",
      "39/132 epoch start with Train Dataloader 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 epoch - Train Loss: 0.3044, Train mIoU: 0.5433\n",
      "39 epoch - Validation Loss: 0.3189, Validation mIoU: 0.5299\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 39 as ./resize_checkpoints/checkpoint_epoch_39.pth.\n",
      "40/132 epoch start with Train Dataloader 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 epoch - Train Loss: 0.3069, Train mIoU: 0.5417\n",
      "40 epoch - Validation Loss: 0.3566, Validation mIoU: 0.5230\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 40 as ./resize_checkpoints/checkpoint_epoch_40.pth.\n",
      "41/132 epoch start with Train Dataloader 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 epoch - Train Loss: 0.2740, Train mIoU: 0.5493\n",
      "41 epoch - Validation Loss: 0.3406, Validation mIoU: 0.5444\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 41 as ./resize_checkpoints/checkpoint_epoch_41.pth.\n",
      "42/132 epoch start with Train Dataloader 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 epoch - Train Loss: 0.3099, Train mIoU: 0.5261\n",
      "42 epoch - Validation Loss: 0.3169, Validation mIoU: 0.5473\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 42 as ./resize_checkpoints/checkpoint_epoch_42.pth.\n",
      "Checkpoint saved at epoch 42 as ./resize_checkpoints/best_checkpoint_epoch_42.pth.\n",
      "43/132 epoch start with Train Dataloader 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 epoch - Train Loss: 0.3014, Train mIoU: 0.5539\n",
      "43 epoch - Validation Loss: 0.3035, Validation mIoU: 0.5402\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 43 as ./resize_checkpoints/checkpoint_epoch_43.pth.\n",
      "44/132 epoch start with Train Dataloader 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 epoch - Train Loss: 0.2955, Train mIoU: 0.5453\n",
      "44 epoch - Validation Loss: 0.3376, Validation mIoU: 0.5355\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 44 as ./resize_checkpoints/checkpoint_epoch_44.pth.\n",
      "45/132 epoch start with Train Dataloader 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 epoch - Train Loss: 0.2702, Train mIoU: 0.5522\n",
      "45 epoch - Validation Loss: 0.3246, Validation mIoU: 0.5439\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 45 as ./resize_checkpoints/checkpoint_epoch_45.pth.\n",
      "46/132 epoch start with Train Dataloader 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 epoch - Train Loss: 0.2993, Train mIoU: 0.5388\n",
      "46 epoch - Validation Loss: 0.3260, Validation mIoU: 0.5320\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 46 as ./resize_checkpoints/checkpoint_epoch_46.pth.\n",
      "47/132 epoch start with Train Dataloader 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 epoch - Train Loss: 0.2945, Train mIoU: 0.5495\n",
      "47 epoch - Validation Loss: 0.3134, Validation mIoU: 0.5493\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 47 as ./resize_checkpoints/checkpoint_epoch_47.pth.\n",
      "Checkpoint saved at epoch 47 as ./resize_checkpoints/best_checkpoint_epoch_47.pth.\n",
      "48/132 epoch start with Train Dataloader 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 epoch - Train Loss: 0.2968, Train mIoU: 0.5560\n",
      "48 epoch - Validation Loss: 0.3706, Validation mIoU: 0.5241\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 48 as ./resize_checkpoints/checkpoint_epoch_48.pth.\n",
      "49/132 epoch start with Train Dataloader 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 epoch - Train Loss: 0.2649, Train mIoU: 0.5540\n",
      "49 epoch - Validation Loss: 0.3158, Validation mIoU: 0.5420\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 49 as ./resize_checkpoints/checkpoint_epoch_49.pth.\n",
      "50/132 epoch start with Train Dataloader 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m checkpoint_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./resize_checkpoints/best_checkpoint_epoch_32.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# 학습 함수 호출\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mtrain_model_with_csv_and_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcsv_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./resize_metrics.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./resize_checkpoints\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./resize_checkpoints/best_checkpoint_epoch_32.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 여기에 원하는 체크포인트 파일을 지정\u001b[39;49;00m\n\u001b[1;32m     38\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 109\u001b[0m, in \u001b[0;36mtrain_model_with_csv_and_checkpoint\u001b[0;34m(model, train_dataloader, val_dataloader, criterion, optimizer, num_classes, num_epochs, device, csv_filename, checkpoint_dir, checkpoint_file)\u001b[0m\n\u001b[1;32m    106\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    108\u001b[0m outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 109\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    110\u001b[0m total_train_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    112\u001b[0m class_iou \u001b[38;5;241m=\u001b[39m calculate_iou_per_class(outputs, masks, num_classes)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# CUDA 환경 설정\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "# 모델 및 학습 설정\n",
    "num_classes = 8  # 실제 학습할 클래스 수 (배경 제외)\n",
    "learning_rate = 0.0001  # 논문에 나와있는 learning rate\n",
    "weight_decay = 0.0001\n",
    "num_epochs = 100\n",
    "\n",
    "\n",
    "# 모델, 옵티마이저, 손실 함수 설정\n",
    "model = LightweightDeepLabV3(num_classes=num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# 손실 함수 및 장치 설정 (가중치 없이)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0).to(device)\n",
    "\n",
    "# 체크포인트 파일 지정 \n",
    "checkpoint_file = \"./resize_checkpoints/best_checkpoint_epoch_32.pth\"\n",
    "\n",
    "# 학습 함수 호출\n",
    "train_model_with_csv_and_checkpoint(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloaders,\n",
    "    val_dataloader=val_dataloader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_classes=num_classes,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    "    csv_filename=\"./resize_metrics.csv\",\n",
    "    checkpoint_dir=\"./resize_checkpoints\",\n",
    "    checkpoint_file=\"./resize_checkpoints/best_checkpoint_epoch_32.pth\"  # 여기에 원하는 체크포인트 파일을 지정\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 배치사이즈 64->128 , 러닝레이트 줄이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloaders = [DataLoader(CustomSegmentationDataset(image_dir, mask_dir, transform=transform), batch_size=128, shuffle=True) \n",
    "                     for image_dir, mask_dir in zip(train_image_dirs, train_mask_dirs)]\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Class names definition\n",
    "CLASS_NAMES = [\"Background\", \"Sidewalk\", \"Braille Guide Blocks\", \"Roadway\", \"Alley\", \"Bike Lane\", \"Caution Zone\", \"Cross walk\"]\n",
    "\n",
    "# 성능 지표를 CSV 파일에 저장하는 함수 (클래스별 mIoU 추가, 배경 클래스 제외)\n",
    "def save_metrics_to_csv(epoch, train_loss, val_loss, train_miou, val_miou, class_train_iou, class_val_iou, filename=\"resize_metrics.csv\"):\n",
    "    try:\n",
    "        file_exists = os.path.isfile(filename)\n",
    "        \n",
    "        with open(filename, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            if not file_exists:\n",
    "                # CSV 헤더에 클래스별 mIoU 추가 (배경 제외)\n",
    "                header = [\"Epoch\", \"Train Loss\", \"Val Loss\", \"Train mIoU\", \"Val mIoU\"] + \\\n",
    "                         [f\"Train IoU - {name}\" for name in CLASS_NAMES[1:]] + \\\n",
    "                         [f\"Val IoU - {name}\" for name in CLASS_NAMES[1:]]\n",
    "                writer.writerow(header)\n",
    "            \n",
    "            # 배경 클래스를 제외한 클래스별 mIoU와 함께 데이터를 작성\n",
    "            row = [epoch, train_loss, val_loss, train_miou, val_miou] + \\\n",
    "                  class_train_iou + class_val_iou\n",
    "            writer.writerow(row)\n",
    "        print(f\"Metrics saved successfully to {filename}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save metrics to CSV: {e}\")\n",
    "\n",
    "# Function to save model checkpoints\n",
    "def save_checkpoint(epoch, model, optimizer, checkpoint_dir, filename=\"last_checkpoint.pth\"):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "    checkpoint_filename = os.path.join(checkpoint_dir, filename)\n",
    "    torch.save(checkpoint, checkpoint_filename)\n",
    "    print(f\"Checkpoint saved at epoch {epoch} as {checkpoint_filename}.\")\n",
    "    print(\"\\n=========================================================================\")\n",
    "    \n",
    "\n",
    "# Function to load model checkpoints\n",
    "def load_checkpoint(filename, model, optimizer=None):\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    print(f\"Checkpoint loaded from epoch {epoch}.\")\n",
    "    return epoch\n",
    "\n",
    "# Function to calculate IoU per class\n",
    "def calculate_iou_per_class(pred, target, num_classes):\n",
    "    iou_per_class = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = (pred_inds & target_inds).sum().float().item()\n",
    "        union = (pred_inds | target_inds).sum().float().item()\n",
    "        if union == 0:\n",
    "            iou_per_class.append(float('nan'))\n",
    "        else:\n",
    "            iou_per_class.append(intersection / union)\n",
    "    return iou_per_class\n",
    "\n",
    "\n",
    "def train_model_with_csv_and_checkpoint(model, train_dataloader, val_dataloader, criterion, optimizer, num_classes, num_epochs=25, device='cuda:0', csv_filename=\"resize_metrics.csv\", checkpoint_dir=\"./resize_checkpoints\", checkpoint_file=None):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    model.to(device)\n",
    "    print(\"모델 장치:\", next(model.parameters()).device)\n",
    "\n",
    "    best_val_miou = -1\n",
    "\n",
    "    # 체크포인트 파일을 불러오고, 이어서 학습할 시작 에폭 설정\n",
    "    start_epoch = 0\n",
    "    if checkpoint_file and os.path.isfile(checkpoint_file):\n",
    "        start_epoch = load_checkpoint(checkpoint_file, model, optimizer)\n",
    "        print(f\"체크포인트 로드 완료. {start_epoch + 1} 에폭부터 재개합니다.\")\n",
    "    else:\n",
    "        print(\"체크포인트가 없거나 파일을 찾을 수 없습니다. 처음부터 학습을 시작합니다.\")\n",
    "\n",
    "    # CSV에 기록할 에폭을 66부터 시작하도록 설정\n",
    "    csv_epoch = 66\n",
    "\n",
    "    # 학습 루프: `start_epoch + 1`부터 `start_epoch + num_epochs`까지\n",
    "    for epoch in range(start_epoch + 1, start_epoch + num_epochs + 1):\n",
    "        dataloader_index = epoch % len(train_dataloader)\n",
    "        current_dataloader = train_dataloader[dataloader_index]\n",
    "        \n",
    "        print(\"\\n=========================================================================\")\n",
    "        print(f\"{epoch}/{start_epoch + num_epochs} epoch start with Train Dataloader {dataloader_index + 1}\")\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_iou = [0.0] * num_classes\n",
    "        total_train_samples = 0\n",
    "\n",
    "        for images, masks in tqdm(current_dataloader, desc=f\"Train Epoch {epoch}, Dataloader {dataloader_index + 1}\", leave=False):\n",
    "            images = images.to(device)\n",
    "            masks = masks.long().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            outputs = torch.argmax(outputs, dim=1)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            total_train_samples += images.size(0)\n",
    "\n",
    "            class_iou = calculate_iou_per_class(outputs, masks, num_classes)\n",
    "            for j in range(num_classes):\n",
    "                if not np.isnan(class_iou[j]):\n",
    "                    train_iou[j] += class_iou[j]\n",
    "\n",
    "        epoch_loss = running_loss / total_train_samples\n",
    "        train_iou = [x / len(current_dataloader) for x in train_iou]\n",
    "        train_miou = np.nanmean(train_iou[1:])\n",
    "\n",
    "        print(f\"{epoch} epoch - Train Loss: {epoch_loss:.4f}, Train mIoU: {train_miou:.4f}\")\n",
    "\n",
    "        # 검증 단계 수행\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_iou = [0.0] * num_classes\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_dataloader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.long().to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                outputs = torch.argmax(outputs, dim=1)\n",
    "                val_running_loss += loss.item() * images.size(0)\n",
    "\n",
    "                class_iou = calculate_iou_per_class(outputs, masks, num_classes)\n",
    "                for j in range(num_classes):\n",
    "                    if not np.isnan(class_iou[j]):\n",
    "                        val_iou[j] += class_iou[j]\n",
    "\n",
    "        val_loss = val_running_loss / len(val_dataloader.dataset)\n",
    "        val_iou = [x / len(val_dataloader) for x in val_iou]\n",
    "        val_miou = np.nanmean(val_iou[1:])\n",
    "\n",
    "        print(f\"{epoch} epoch - Validation Loss: {val_loss:.4f}, Validation mIoU: {val_miou:.4f}\")\n",
    "        \n",
    "\n",
    "        # 강제로 CSV에 기록할 에폭을 49부터 시작하도록 설정\n",
    "        save_metrics_to_csv(csv_epoch, epoch_loss, val_loss, train_miou, val_miou, train_iou[1:], val_iou[1:])\n",
    "        csv_epoch += 1  # CSV 에폭을 증가시켜 다음 에폭에 반영\n",
    "\n",
    "        # 매 에폭마다 체크포인트 저장\n",
    "        epoch_checkpoint_filename = f\"checkpoint_epoch_{csv_epoch}.pth\"\n",
    "        save_checkpoint(csv_epoch, model, optimizer, checkpoint_dir, filename=epoch_checkpoint_filename)\n",
    "\n",
    "        # 최적의 검증 성능 기준으로 최상의 체크포인트도 업데이트\n",
    "        if val_miou > best_val_miou:\n",
    "            best_val_miou = val_miou\n",
    "            best_checkpoint_filename = f\"best_checkpoint_epoch_{csv_epoch}.pth\"\n",
    "            save_checkpoint(csv_epoch, model, optimizer, checkpoint_dir, filename=best_checkpoint_filename)\n",
    "\n",
    "    # 마지막 에폭 체크포인트 저장\n",
    "    last_checkpoint_filename = f\"last_checkpoint_epoch_{csv_epoch}.pth\"\n",
    "    save_checkpoint(csv_epoch, model, optimizer, checkpoint_dir, filename=last_checkpoint_filename)\n",
    "    print(\"훈련 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 장치: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2775340/882447347.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded from epoch 49.\n",
      "체크포인트 로드 완료. 50 에폭부터 재개합니다.\n",
      "\n",
      "=========================================================================\n",
      "50/149 epoch start with Train Dataloader 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 epoch - Train Loss: 0.2765, Train mIoU: 0.5619\n",
      "50 epoch - Validation Loss: 0.2979, Validation mIoU: 0.5714\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 67 as ./resize_checkpoints/checkpoint_epoch_67.pth.\n",
      "\n",
      "=========================================================================\n",
      "Checkpoint saved at epoch 67 as ./resize_checkpoints/best_checkpoint_epoch_67.pth.\n",
      "\n",
      "=========================================================================\n",
      "\n",
      "=========================================================================\n",
      "51/149 epoch start with Train Dataloader 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 epoch - Train Loss: 0.2555, Train mIoU: 0.5840\n",
      "51 epoch - Validation Loss: 0.2918, Validation mIoU: 0.5739\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 68 as ./resize_checkpoints/checkpoint_epoch_68.pth.\n",
      "\n",
      "=========================================================================\n",
      "Checkpoint saved at epoch 68 as ./resize_checkpoints/best_checkpoint_epoch_68.pth.\n",
      "\n",
      "=========================================================================\n",
      "\n",
      "=========================================================================\n",
      "52/149 epoch start with Train Dataloader 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 epoch - Train Loss: 0.2509, Train mIoU: 0.5807\n",
      "52 epoch - Validation Loss: 0.3212, Validation mIoU: 0.5638\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 69 as ./resize_checkpoints/checkpoint_epoch_69.pth.\n",
      "\n",
      "=========================================================================\n",
      "\n",
      "=========================================================================\n",
      "53/149 epoch start with Train Dataloader 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 epoch - Train Loss: 0.2166, Train mIoU: 0.5904\n",
      "53 epoch - Validation Loss: 0.3139, Validation mIoU: 0.5734\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 70 as ./resize_checkpoints/checkpoint_epoch_70.pth.\n",
      "\n",
      "=========================================================================\n",
      "\n",
      "=========================================================================\n",
      "54/149 epoch start with Train Dataloader 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 epoch - Train Loss: 0.2524, Train mIoU: 0.5735\n",
      "54 epoch - Validation Loss: 0.3133, Validation mIoU: 0.5739\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 71 as ./resize_checkpoints/checkpoint_epoch_71.pth.\n",
      "\n",
      "=========================================================================\n",
      "\n",
      "=========================================================================\n",
      "55/149 epoch start with Train Dataloader 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 epoch - Train Loss: 0.2447, Train mIoU: 0.5881\n",
      "55 epoch - Validation Loss: 0.3070, Validation mIoU: 0.5769\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 72 as ./resize_checkpoints/checkpoint_epoch_72.pth.\n",
      "\n",
      "=========================================================================\n",
      "Checkpoint saved at epoch 72 as ./resize_checkpoints/best_checkpoint_epoch_72.pth.\n",
      "\n",
      "=========================================================================\n",
      "\n",
      "=========================================================================\n",
      "56/149 epoch start with Train Dataloader 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 epoch - Train Loss: 0.2461, Train mIoU: 0.5862\n",
      "56 epoch - Validation Loss: 0.3752, Validation mIoU: 0.5508\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 73 as ./resize_checkpoints/checkpoint_epoch_73.pth.\n",
      "\n",
      "=========================================================================\n",
      "\n",
      "=========================================================================\n",
      "57/149 epoch start with Train Dataloader 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 epoch - Train Loss: 0.2233, Train mIoU: 0.5858\n",
      "57 epoch - Validation Loss: 0.3800, Validation mIoU: 0.5513\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 74 as ./resize_checkpoints/checkpoint_epoch_74.pth.\n",
      "\n",
      "=========================================================================\n",
      "\n",
      "=========================================================================\n",
      "58/149 epoch start with Train Dataloader 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 epoch - Train Loss: 0.2576, Train mIoU: 0.5693\n",
      "58 epoch - Validation Loss: 0.3484, Validation mIoU: 0.5421\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 75 as ./resize_checkpoints/checkpoint_epoch_75.pth.\n",
      "\n",
      "=========================================================================\n",
      "\n",
      "=========================================================================\n",
      "59/149 epoch start with Train Dataloader 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 epoch - Train Loss: 0.2492, Train mIoU: 0.5826\n",
      "59 epoch - Validation Loss: 0.2925, Validation mIoU: 0.5748\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 76 as ./resize_checkpoints/checkpoint_epoch_76.pth.\n",
      "\n",
      "=========================================================================\n",
      "\n",
      "=========================================================================\n",
      "60/149 epoch start with Train Dataloader 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 epoch - Train Loss: 0.2450, Train mIoU: 0.5856\n",
      "60 epoch - Validation Loss: 0.3455, Validation mIoU: 0.5397\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 77 as ./resize_checkpoints/checkpoint_epoch_77.pth.\n",
      "\n",
      "=========================================================================\n",
      "\n",
      "=========================================================================\n",
      "61/149 epoch start with Train Dataloader 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 epoch - Train Loss: 0.2190, Train mIoU: 0.5825\n",
      "61 epoch - Validation Loss: 0.3347, Validation mIoU: 0.5656\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 78 as ./resize_checkpoints/checkpoint_epoch_78.pth.\n",
      "\n",
      "=========================================================================\n",
      "\n",
      "=========================================================================\n",
      "62/149 epoch start with Train Dataloader 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 epoch - Train Loss: 0.2559, Train mIoU: 0.5752\n",
      "62 epoch - Validation Loss: 0.3346, Validation mIoU: 0.5561\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 79 as ./resize_checkpoints/checkpoint_epoch_79.pth.\n",
      "\n",
      "=========================================================================\n",
      "\n",
      "=========================================================================\n",
      "63/149 epoch start with Train Dataloader 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 epoch - Train Loss: 0.2513, Train mIoU: 0.5870\n",
      "63 epoch - Validation Loss: 0.2989, Validation mIoU: 0.5652\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 80 as ./resize_checkpoints/checkpoint_epoch_80.pth.\n",
      "\n",
      "=========================================================================\n",
      "\n",
      "=========================================================================\n",
      "64/149 epoch start with Train Dataloader 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 epoch - Train Loss: 0.2514, Train mIoU: 0.5825\n",
      "64 epoch - Validation Loss: 0.3375, Validation mIoU: 0.5408\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 81 as ./resize_checkpoints/checkpoint_epoch_81.pth.\n",
      "\n",
      "=========================================================================\n",
      "\n",
      "=========================================================================\n",
      "65/149 epoch start with Train Dataloader 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 epoch - Train Loss: 0.2230, Train mIoU: 0.5826\n",
      "65 epoch - Validation Loss: 0.3384, Validation mIoU: 0.5608\n",
      "Metrics saved successfully to resize_metrics.csv.\n",
      "Checkpoint saved at epoch 82 as ./resize_checkpoints/checkpoint_epoch_82.pth.\n",
      "\n",
      "=========================================================================\n",
      "\n",
      "=========================================================================\n",
      "66/149 epoch start with Train Dataloader 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m checkpoint_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./resize_checkpoints/checkpoint_epoch_49.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# 학습 함수 호출\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mtrain_model_with_csv_and_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcsv_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./resize_metrics.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./resize_checkpoints\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_file\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[44], line 101\u001b[0m, in \u001b[0;36mtrain_model_with_csv_and_checkpoint\u001b[0;34m(model, train_dataloader, val_dataloader, criterion, optimizer, num_classes, num_epochs, device, csv_filename, checkpoint_dir, checkpoint_file)\u001b[0m\n\u001b[1;32m     98\u001b[0m train_iou \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.0\u001b[39m] \u001b[38;5;241m*\u001b[39m num_classes\n\u001b[1;32m     99\u001b[0m total_train_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, masks \u001b[38;5;129;01min\u001b[39;00m tqdm(current_dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Dataloader \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataloader_index\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    102\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    103\u001b[0m     masks \u001b[38;5;241m=\u001b[39m masks\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[27], line 29\u001b[0m, in \u001b[0;36mCustomSegmentationDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m     28\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n\u001b[0;32m---> 29\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNEAREST\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# 마스크는 PIL 이미지를 NumPy로 변환 후 텐서로 변환\u001b[39;00m\n\u001b[1;32m     32\u001b[0m mask_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(mask, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/PIL/Image.py:2293\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2290\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreducing_gap must be 1.0 or greater\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m-> 2293\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m box \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2295\u001b[0m     box \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/PIL/ImageFile.py:293\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    292\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 293\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# CUDA 환경 설정\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "# 모델 및 학습 설정\n",
    "num_classes = 8  # 실제 학습할 클래스 수 (배경 제외)\n",
    "learning_rate = 0.00005  \n",
    "weight_decay = 0.0005\n",
    "num_epochs = 100\n",
    "\n",
    "\n",
    "# 모델, 옵티마이저, 손실 함수 설정\n",
    "model = LightweightDeepLabV3(num_classes=num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# 손실 함수 및 장치 설정 (가중치 없이)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0).to(device)\n",
    "\n",
    "# 체크포인트 파일 지정 \n",
    "checkpoint_file = \"./resize_checkpoints/checkpoint_epoch_49.pth\"\n",
    "\n",
    "# 학습 함수 호출\n",
    "train_model_with_csv_and_checkpoint(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloaders,\n",
    "    val_dataloader=val_dataloader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_classes=num_classes,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    "    csv_filename=\"./resize_metrics.csv\",\n",
    "    checkpoint_dir=\"./resize_checkpoints\",\n",
    "    checkpoint_file=checkpoint_file\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
